{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "streamlit_MH_app.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyORkHX+CKcFdXu8fm3vJ1ZT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gail529/Mental-health/blob/main/streamlit_MH_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMwYQokIp26T"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrUsE0eBmDf2"
      },
      "source": [
        "import streamlit as st\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import gensim\n",
        "import os\n",
        "import tweepy as tw\n",
        " \n",
        "#tweet preprocessing \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "import re\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize,TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        " \n",
        " \n",
        " \n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7QkOQyorV6P"
      },
      "source": [
        "#lowercasing and url,punctuations and numbers removal,\n",
        "def Lowercasing(words):\n",
        "    string=re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\",str(words))\n",
        "    word=string.lower()\n",
        "    return word\n",
        "\n",
        "#Tokenization and (@)handle extraction\n",
        "def Tokenization(tweet):\n",
        "    tokenizer=TweetTokenizer(strip_handles=True)\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "    return tokens\n",
        "\n",
        "#punctuations\n",
        "def Punctuation_removal(tokens):\n",
        "    words=[ word for word in tokens if word.isalnum()]\n",
        "    return words\n",
        "\n",
        "#stemming\n",
        "def stemming(text):\n",
        "    stemmer=PorterStemmer()\n",
        "    for  word in text:\n",
        "        stemmed_words=stemmer.stem(word)\n",
        "        return stemmed_words\n",
        "\n",
        "#stopword_removal\n",
        "def remove_stopwords(words):\n",
        "    stop_words=set(stopwords.words(\"english\")) \n",
        "    result=[word for word in words if word not in stop_words ]\n",
        "    return result\n",
        "\n",
        "\n",
        "#lemmatization\n",
        "def lemmatization(text):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    lemmatized_phrase=[]\n",
        "    for word in text:\n",
        "        lemmatized_word=lemmatizer.lemmatize(word)\n",
        "        lemmatized_phrase.append(lemmatized_word)\n",
        "    return lemmatized_phrase\n",
        " \n",
        "\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    tweet_tokens=Tokenization(tweet)\n",
        "    lemmatized_tweet=lemmatization(tweet_tokens)#lemmatization\n",
        "    lowercased_string=Lowercasing(lemmatized_tweet)#lowercasing and removing numbers\n",
        "    return lowercased_string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZd1wJPnmwcI"
      },
      "source": [
        "def predict(message):\n",
        "    model=load_model('/content/MentalHealth_model.h5')\n",
        "    with open('/content/tokenizer.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "    data = clean_tweet(message)\n",
        "    seq=tokenizer.texts_to_sequences(data)\n",
        "    data = pad_sequences(seq,maxlen=200)\n",
        "    prediction=model.predict(data)\n",
        "    output = prediction.argmax(axis=-1)[0]\n",
        "    return output\n",
        "\n",
        " \n",
        "st.title('Depressive tweets detector')\n",
        "message = st.text_area('Enter/paste Tweet,”Type Here ..')\n",
        "if st.button('Analyze'):\n",
        "    with st.spinner('Analyzing the text …'):\n",
        "        prediction=predict(message)\n",
        "    if prediction_class == 0:\n",
        "        st.success()\n",
        "    if prediction_class == 1:\n",
        "        st.success()\n",
        "    if prediction_class == 2:\n",
        "        st.success()\n",
        "    if prediction_class == 3:\n",
        "        st.success()\n",
        "    if prediction_class == 4:\n",
        "        st.success()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-UNdK-yCTLB"
      },
      "source": [
        "# Streamlit code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rl66cqiRQcl"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_1Tz-ZLjwAh"
      },
      "source": [
        "from keras import backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "wordnet = WordNetLemmatizer()\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKqPfmfQAWyE"
      },
      "source": [
        "MODEL_PATH = '/content/MH/saved_model.pb'\n",
        "MAX_NB_WORDS = 100000 # max no. of words for tokenizer\n",
        "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence), including padding\n",
        "VALIDATION_SPLIT = 0.2 # data for validation (not used in training)\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer_file = 'tokenizer.pickle'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plfKV0oIB_8b"
      },
      "source": [
        "with open(tokenizer_file, 'rb') as handle:\n",
        "   tokenizer = pickle.load(handle)\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def Load_model():\n",
        "   model = reconstructed_model\n",
        "   model.summary() # included to make it visible when model is reloaded\n",
        "   session = K.get_session()\n",
        "   return model, session\n",
        "\n",
        "   Load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JV1FqeRTCkl"
      },
      "source": [
        "#@st.cache\n",
        "#def load_my_model():\n",
        "model = reconstructed_model\n",
        " \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    st.title('A simple streamlit mental health app')\n",
        "    st.write('Want to know if your tweets contain some depressive characteristics')\n",
        "    st.subheader('Key in or paste one of your tweets') \n",
        "    tweet=st.text_area('') \n",
        "    prediction_btn = st.button('predict')\n",
        "    if prediction_btn:\n",
        "        clean_text = []\n",
        "        #K.set_session(session)\n",
        "        i = clean_tweet(tweet)\n",
        "        clean_text.append(i)\n",
        "        sequences = tokenizer.texts_to_sequences(clean_text)\n",
        "        data = pad_sequences(sequences, maxlen = max_length)\n",
        "        prediction = model.predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsXUSSw_CG2y"
      },
      "source": [
        "if prediction_btn:\n",
        "   clean_text = []\n",
        "   K.set_session(session)\n",
        "   i = clean_tweet(tweet)\n",
        "   clean_text.append(i)\n",
        "   sequences = tokenizer.texts_to_sequences(clean_text)\n",
        "   data = pad_sequences(sequences, maxlen = max_length)\n",
        "   prediction = model.predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_KjZmvCOLYp"
      },
      "source": [
        "prediction_class = prediction.argmax(axis=-1)[0]\n",
        "\n",
        "\n",
        "st.header(\"Prediction using LSTM model\")\n",
        "if prediction_class == 0:\n",
        "    st.success()\n",
        "if prediction_class == 1:\n",
        "    st.success()\n",
        "if prediction_class == 2:\n",
        "    st.success()\n",
        "if prediction_class == 3:\n",
        "    st.success()\n",
        "if prediction_class == 4:\n",
        "    st.success()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBA-YKHqQAu3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrPDCr5NJVEF"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    st.title('A simple streamlit mental health app')\n",
        "    st.write('Want to know if your tweets contain some depressive characteristics')\n",
        "    st.subheader('Key in or paste one of your tweets') \n",
        "    tweet=st.text_area('') \n",
        "    prediction_btn = st.button('predict')\n",
        "    model= Load_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}