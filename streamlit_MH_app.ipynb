{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "streamlit_MH_app.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBM+wQXIF1wmTa6YlwE81W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gail529/Mental-health/blob/main/streamlit_MH_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMwYQokIp26T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987fc5e3-d88f-41f3-f3c3-0516f29d539f"
      },
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok==4.1.1\n",
        "!pip install ngrok"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (0.82.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.6.2)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.14)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.2)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.12.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.5.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (4.0.7)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (57.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit) (4.0.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.1.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.10.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Collecting pyngrok==4.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/a9/de2e15c92eb3aa4a2646ce3a7542317eb69ac47f667578ce8bf916320847/pyngrok-4.1.1.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-cp37-none-any.whl size=15985 sha256=8b1e617316aae8aad47f4f04184dccf28940f7489d0851caf42b1c1fb94ae9c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/71/0d/1695f7c8815c0beb3b5d9b35d6eec9243c87e6070fbe3977fa\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n",
            "Collecting ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/67/50/c64abd519f50537df4726bc2572862d1925e377d4a80a9c06e70db048682/ngrok-0.1.6.tar.gz\n",
            "Collecting ping\n",
            "  Downloading https://files.pythonhosted.org/packages/03/ac/9a3f332f8d7d27cd8929922b2e225842c1623760364b37fc79dc0f65e77d/ping-0.2.tar.gz\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrUsE0eBmDf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d271e6-2a9a-4f23-8e5f-a0c5b9a0aa29"
      },
      "source": [
        "import streamlit as st\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import gensim\n",
        "import os\n",
        "import tweepy as tw\n",
        " \n",
        "#tweet preprocessing \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "import re\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize,TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        " \n",
        " \n",
        " \n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-16 04:11:22.517 INFO    summarizer.preprocessing.cleaner: 'pattern' package not found; tag filters are not available for English\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7QkOQyorV6P"
      },
      "source": [
        "#lowercasing and url,punctuations and numbers removal,\n",
        "def Lowercasing(words):\n",
        "    string=re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\",str(words))\n",
        "    word=string.lower()\n",
        "    return word\n",
        "\n",
        "#Tokenization and (@)handle extraction\n",
        "def Tokenization(tweet):\n",
        "    tokenizer=TweetTokenizer(strip_handles=True)\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "    return tokens\n",
        "\n",
        "#punctuations\n",
        "def Punctuation_removal(tokens):\n",
        "    words=[ word for word in tokens if word.isalnum()]\n",
        "    return words\n",
        "\n",
        "#stemming\n",
        "def stemming(text):\n",
        "    stemmer=PorterStemmer()\n",
        "    for  word in text:\n",
        "        stemmed_words=stemmer.stem(word)\n",
        "        return stemmed_words\n",
        "\n",
        "#stopword_removal\n",
        "def remove_stopwords(words):\n",
        "    stop_words=set(stopwords.words(\"english\")) \n",
        "    result=[word for word in words if word not in stop_words ]\n",
        "    return result\n",
        "\n",
        "\n",
        "#lemmatization\n",
        "def lemmatization(text):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    lemmatized_phrase=[]\n",
        "    for word in text:\n",
        "        lemmatized_word=lemmatizer.lemmatize(word)\n",
        "        lemmatized_phrase.append(lemmatized_word)\n",
        "    return lemmatized_phrase\n",
        " \n",
        "\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    tweet_tokens=Tokenization(tweet)\n",
        "    lemmatized_tweet=lemmatization(tweet_tokens)#lemmatization\n",
        "    lowercased_string=Lowercasing(lemmatized_tweet)#lowercasing and removing numbers\n",
        "    return lowercased_string\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZd1wJPnmwcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5993884f-c973-4267-8013-7229c1152917"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import gensim\n",
        "import os\n",
        "import tweepy as tw\n",
        " \n",
        "#tweet preprocessing \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "import re\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize,TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        " \n",
        " \n",
        " \n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def Lowercasing(words):\n",
        "    string=re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\",str(words))\n",
        "    word=string.lower()\n",
        "    return word\n",
        "\n",
        "#Tokenization and (@)handle extraction\n",
        "def Tokenization(tweet):\n",
        "    tokenizer=TweetTokenizer(strip_handles=True)\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "    return tokens\n",
        "\n",
        "#punctuations\n",
        "def Punctuation_removal(tokens):\n",
        "    words=[ word for word in tokens if word.isalnum()]\n",
        "    return words\n",
        "\n",
        "#stemming\n",
        "def stemming(text):\n",
        "    stemmer=PorterStemmer()\n",
        "    for  word in text:\n",
        "        stemmed_words=stemmer.stem(word)\n",
        "        return stemmed_words\n",
        "\n",
        "#stopword_removal\n",
        "def remove_stopwords(words):\n",
        "    stop_words=set(stopwords.words(\"english\")) \n",
        "    result=[word for word in words if word not in stop_words ]\n",
        "    return result\n",
        "\n",
        "\n",
        "#lemmatization\n",
        "def lemmatization(text):\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    lemmatized_phrase=[]\n",
        "    for word in text:\n",
        "        lemmatized_word=lemmatizer.lemmatize(word)\n",
        "        lemmatized_phrase.append(lemmatized_word)\n",
        "    return lemmatized_phrase\n",
        " \n",
        "\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    tweet_tokens=Tokenization(tweet)\n",
        "    lemmatized_tweet=lemmatization(tweet_tokens)#lemmatization\n",
        "    lowercased_string=Lowercasing(lemmatized_tweet)#lowercasing and removing numbers\n",
        "    return lowercased_string\n",
        "\n",
        "\n",
        "def predict(message):\n",
        "    model=load_model('/content/MentalHealth_model.h5')\n",
        "    with open('/content/tokenizer.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "    data = clean_tweet(message)\n",
        "    seq=tokenizer.texts_to_sequences(data)\n",
        "    data = pad_sequences(seq,maxlen=200)\n",
        "    prediction=model.predict(data)\n",
        "    output = prediction.argmax(axis=1)[0][0]\n",
        "    return output\n",
        "\n",
        " \n",
        "st.title('Depressive tweets detector')\n",
        "message = st.text_area('Enter/paste Tweet,”Type Here ..')\n",
        "if st.button('Analyze'):\n",
        "    with st.spinner('Analyzing the text …'):\n",
        "        prediction=predict(message)\n",
        "        if prediction == 0:\n",
        "            st.success('Most likely to be depressed')\n",
        "        if prediction == 1:\n",
        "            st.success('quite close to being depressed')\n",
        "        if prediction == 2:\n",
        "            st.success('neutral')\n",
        "        if prediction == 3:\n",
        "            st.success('not likely to be depressed')\n",
        "        if prediction == 4:\n",
        "            st.success('definitely not  depressed')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pewDDS9MBOzY",
        "outputId": "8bf8378a-b5f3-4857-f04b-7fa3f5d772d5"
      },
      "source": [
        "! ngrok authtoken 1n3n9zfRI524JaNlhCuS3GHARgM_7Z465eCAfxd5tsaavs7Tj\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtboktgzGNed",
        "outputId": "ba3f966f-4e2c-49f7-fa93-29732fce451f"
      },
      "source": [
        "! ngrok"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME:\n",
            "   ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "DESCRIPTION:\n",
            "    ngrok exposes local networked services behinds NATs and firewalls to the\n",
            "    public internet over a secure tunnel. Share local websites, build/test\n",
            "    webhook consumers and self-host personal services.\n",
            "    Detailed help for each command is available with 'ngrok help <command>'.\n",
            "    Open http://localhost:4040 for ngrok's web interface to inspect traffic.\n",
            "\n",
            "EXAMPLES:\n",
            "    ngrok http 80                    # secure public URL for port 80 web server\n",
            "    ngrok http -subdomain=baz 8080   # port 8080 available at baz.ngrok.io\n",
            "    ngrok http foo.dev:80            # tunnel to host:port instead of localhost\n",
            "    ngrok http https://localhost     # expose a local https server\n",
            "    ngrok tcp 22                     # tunnel arbitrary TCP traffic to port 22\n",
            "    ngrok tls -hostname=foo.com 443  # TLS traffic for foo.com to port 443\n",
            "    ngrok start foo bar baz          # start tunnels from the configuration file\n",
            "\n",
            "VERSION:\n",
            "   2.3.40\n",
            "\n",
            "AUTHOR:\n",
            "  inconshreveable - <alan@ngrok.com>\n",
            "\n",
            "COMMANDS:\n",
            "   authtoken\tsave authtoken to configuration file\n",
            "   credits\tprints author and licensing information\n",
            "   http\t\tstart an HTTP tunnel\n",
            "   start\tstart tunnels by name from the configuration file\n",
            "   tcp\t\tstart a TCP tunnel\n",
            "   tls\t\tstart a TLS tunnel\n",
            "   update\tupdate ngrok to the latest version\n",
            "   version\tprint the version string\n",
            "   help\t\tShows a list of commands or help for one command\n",
            "\n",
            "PYNGROK VERSION:\n",
            "   4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J1veDOpGTxt"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "pkPcu5vbGTtf",
        "outputId": "959bcea5-ec72-4917-a2ad-cbd0632dc01b"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-16 04:35:01.787 INFO    pyngrok.process: ngrok process starting: 451\n",
            "2021-06-16 04:35:01.808 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "\n",
            "2021-06-16 04:35:01.810 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "\n",
            "2021-06-16 04:35:01.818 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "\n",
            "2021-06-16 04:35:01.824 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "\n",
            "2021-06-16 04:35:01.983 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "\n",
            "2021-06-16 04:35:01.985 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=\"client session established\" obj=csess id=3938e892f1a2\n",
            "\n",
            "2021-06-16 04:35:01.996 INFO    pyngrok.process: ngrok process has started: http://127.0.0.1:4040\n",
            "2021-06-16 04:35:02.003 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=start pg=/api/tunnels id=a310be0a82c8d436\n",
            "\n",
            "2021-06-16 04:35:02.005 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=end pg=/api/tunnels id=a310be0a82c8d436 status=200 dur=396.696µs\n",
            "\n",
            "2021-06-16 04:35:02.010 INFO    pyngrok.process: t=2021-06-16T04:35:01+0000 lvl=info msg=start pg=/api/tunnels id=140b4e3caf5589ce\n",
            "\n",
            "2021-06-16 04:35:02.015 INFO    pyngrok.process: t=2021-06-16T04:35:02+0000 lvl=info msg=end pg=/api/tunnels id=140b4e3caf5589ce status=200 dur=1.509095ms\n",
            "\n",
            "2021-06-16 04:35:02.017 INFO    pyngrok.process: t=2021-06-16T04:35:02+0000 lvl=info msg=start pg=/api/tunnels id=8e0bbe4f4a456634\n",
            "\n",
            "2021-06-16 04:35:02.139 INFO    pyngrok.process: t=2021-06-16T04:35:02+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=\"http-8501-f848648b-bebc-4bb9-a4d6-e23b698d6a28 (http)\" addr=http://localhost:8501 url=http://634c9188509e.ngrok.io\n",
            "\n",
            "2021-06-16 04:35:02.147 INFO    pyngrok.process: t=2021-06-16T04:35:02+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8501-f848648b-bebc-4bb9-a4d6-e23b698d6a28 addr=http://localhost:8501 url=https://634c9188509e.ngrok.io\n",
            "\n",
            "2021-06-16 04:35:02.149 INFO    pyngrok.process: t=2021-06-16T04:35:02+0000 lvl=info msg=end pg=/api/tunnels id=8e0bbe4f4a456634 status=201 dur=137.427389ms\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://634c9188509e.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-UNdK-yCTLB"
      },
      "source": [
        "# Streamlit code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rl66cqiRQcl"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_1Tz-ZLjwAh"
      },
      "source": [
        "from keras import backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "wordnet = WordNetLemmatizer()\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKqPfmfQAWyE"
      },
      "source": [
        "MODEL_PATH = '/content/MH/saved_model.pb'\n",
        "MAX_NB_WORDS = 100000 # max no. of words for tokenizer\n",
        "MAX_SEQUENCE_LENGTH = 200 # max length of each entry (sentence), including padding\n",
        "VALIDATION_SPLIT = 0.2 # data for validation (not used in training)\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer_file = 'tokenizer.pickle'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plfKV0oIB_8b"
      },
      "source": [
        "with open(tokenizer_file, 'rb') as handle:\n",
        "   tokenizer = pickle.load(handle)\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def Load_model():\n",
        "   model = reconstructed_model\n",
        "   model.summary() # included to make it visible when model is reloaded\n",
        "   session = K.get_session()\n",
        "   return model, session\n",
        "\n",
        "   Load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JV1FqeRTCkl"
      },
      "source": [
        "#@st.cache\n",
        "#def load_my_model():\n",
        "model = reconstructed_model\n",
        " \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    st.title('A simple streamlit mental health app')\n",
        "    st.write('Want to know if your tweets contain some depressive characteristics')\n",
        "    st.subheader('Key in or paste one of your tweets') \n",
        "    tweet=st.text_area('') \n",
        "    prediction_btn = st.button('predict')\n",
        "    if prediction_btn:\n",
        "        clean_text = []\n",
        "        #K.set_session(session)\n",
        "        i = clean_tweet(tweet)\n",
        "        clean_text.append(i)\n",
        "        sequences = tokenizer.texts_to_sequences(clean_text)\n",
        "        data = pad_sequences(sequences, maxlen = max_length)\n",
        "        prediction = model.predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsXUSSw_CG2y"
      },
      "source": [
        "if prediction_btn:\n",
        "   clean_text = []\n",
        "   K.set_session(session)\n",
        "   i = clean_tweet(tweet)\n",
        "   clean_text.append(i)\n",
        "   sequences = tokenizer.texts_to_sequences(clean_text)\n",
        "   data = pad_sequences(sequences, maxlen = max_length)\n",
        "   prediction = model.predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_KjZmvCOLYp"
      },
      "source": [
        "prediction_class = prediction.argmax(axis=-1)[0]\n",
        "\n",
        "\n",
        "st.header(\"Prediction using LSTM model\")\n",
        "if prediction_class == 0:\n",
        "    st.success()\n",
        "if prediction_class == 1:\n",
        "    st.success()\n",
        "if prediction_class == 2:\n",
        "    st.success()\n",
        "if prediction_class == 3:\n",
        "    st.success()\n",
        "if prediction_class == 4:\n",
        "    st.success()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBA-YKHqQAu3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrPDCr5NJVEF"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    st.title('A simple streamlit mental health app')\n",
        "    st.write('Want to know if your tweets contain some depressive characteristics')\n",
        "    st.subheader('Key in or paste one of your tweets') \n",
        "    tweet=st.text_area('') \n",
        "    prediction_btn = st.button('predict')\n",
        "    model= Load_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}